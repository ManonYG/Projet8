{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\manon\\\\OneDrive\\\\Documents\\\\OpenClassrooms\\\\Projet8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "test = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D'après \"Comparison Between 5 Regression Algorithm\", les forêts aléatoires semblent particulièrement indiquées pour ces données. C'est donc le modèle sur lequel nous nous pencherons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    param = {\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        'min_child_weight':1,\n",
    "        'colsample_bytree':0.81,\n",
    "        'seed':45,\n",
    "        'reg_alpha':1,#1e-03,\n",
    "        'reg_lambda':0,\n",
    "        'nthread':-1,\n",
    "    }\n",
    "    \n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "#          param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n",
    "        param[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)\n",
    "        #param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "        param[\"learning_rate\"] = trial.suggest_float('learning_rate', 0.01, 0.11)\n",
    "        param[\"subsample\"] = trial.suggest_float('subsample', 0.01, 0.11)\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop\", 1e-8, 1.0)\n",
    "        param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "    bst = xgb.train(param, d_train, evals=[(d_test, \"validation\")], early_stopping_rounds=300,callbacks=[pruning_callback], maximize=False)\n",
    "    preds = bst.predict(d_valid)\n",
    "    rmse_score = sklearn.metrics.mean_squared_error(y_valid, preds, squared=True)\n",
    "    return rmse_score\n",
    "    \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('RMSE: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
